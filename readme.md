# Probably Interesting Data

### Submission by Benjamin Wyss

Estimating dataset distributions and generating new data points by modeling data as k-means clusters

## Datasets Examined

[Wine Dataset](https://archive.ics.uci.edu/ml/datasets/Wine)

This dataset contains 13 continuous numeric attributes with a target goal of determining which of three cultivars a given wine is derived from.

[Car Evaluation Dataset](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)

This dataset contains 6 discrete string attributes with a target goal of determining which of four acceptability classes a given car belongs to. For this project, discrete string attributes are converted to discrete numeric attributes so that there exists a notion of numeric distance between data points. Examine [carDataToNumeric.py](https://github.com/benjaminwyss/EECS738ProbablyInterestingData/blob/main/carDataToNumeric.py) to see how [data/car.data](https://github.com/benjaminwyss/EECS738ProbablyInterestingData/blob/main/data/car.data) is converted to [data/carNumeric.data](https://github.com/benjaminwyss/EECS738ProbablyInterestingData/blob/main/data/carNumeric.data).

## The Big Idea

The k-means algorithm groups data points together based on closeness of features. I hypothesize that the examined datasets will have clusters of points that are similarly distributed, and as such k-means can model the datasets as distinct clusters of similar points. Using the determined clusters, new data points can be generated by calculating the value of a random variable that is either discrete or continuous within the feature ranges of a given cluster.

## Process

The k-means algorithm is implemented in [kmeans.py](https://github.com/benjaminwyss/EECS738ProbablyInterestingData/blob/main/kmeans.py). The file includes methods to fit clusters to a given dataset, predict the clusters of additional data, and generate new data points--either continuous or discrete. The main driver code is located in [main.py](https://github.com/benjaminwyss/EECS738ProbablyInterestingData/blob/main/main.py), and is responsible for reading in a given dataset, fitting k-means clusters for the dataset, generating new data points, and plotting the results (The x and y axis of the generated plot are determined to be the two features which have the greatest average intercluster distance). To determine the dataset's number of clusters (k), the first column of the dataset is treated as the data's classes, thus k is determined to be the number of unique values within the dataset's first column. To run the project, execute the main file in the following format:

`py main.py <path_to_dataset> <newly generated data mode--options are: continuous|discrete|none>`

The last argument is used to determine how new data points will be generated. Use 'none' to not generate any new data points, 'continuous' if the dataset contains continuous numeric features, and 'discrete' if the dataset contains discrete numeric features.

## Results

Using k-means clusters to model the distributions within a dataset relies on the assumption that data points belonging to the same class are close together in *distance*. This assumption holds well for datasets like the [Wine Dataset](https://archive.ics.uci.edu/ml/datasets/Wine), but less so for datasets like the [Car Evaluation Dataset](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation).

Here is the actual class distribution of the wine dataset:

![Image of Wine Dataset Classes](images/wineActual.png)

And this shows how data points are clustered by the k-means algorithm:

![Image of Wine Dataset Clusters](images/wineClusters.png)
